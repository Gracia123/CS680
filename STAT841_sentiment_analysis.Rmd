---
title: "STAT841_6.4"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
```

#a)
```{r}
library(quanteda)
library(dplyr)
library(e1071)
library(caret)
library(tidyverse)
library(MASS)
library(randomForest)
set.seed(7419)
filepath <- "/Users/graciagodwin/Desktop/sentiment labelled sentences"
data<-read.table(paste0(filepath,"/amazon_cells_labelled.txt"), sep="\t", 
                 header=FALSE, stringsAsFactor = FALSE)
colnames(data) <- c('Text','Review')
text<-data$Text
toks <- tokens(x=text,what="word",remove_punct=TRUE,remove_symbols=TRUE,
               remove_numbers=TRUE,remove_url=TRUE)
toks<-tokens_tolower(toks)
toks<-tokens_wordstem(toks)
sw <- stopwords("english")
toks<-tokens_remove(toks, sw)
dtm <- dfm(toks,tolower=TRUE,stem=TRUE,remove=sw)
sparsity(dtm)
summary(colSums(dtm))
tf_total <- colSums(dtm)
to_keep <- which(tf_total > median(tf_total))
dtm_reduced <- dfm_keep(dtm, pattern = names(to_keep), valuetype = "fixed", 
                        verbose = TRUE)
dfreq <- apply(dtm_reduced, 2, function(x) sum(x>0))
summary(dfreq)

#Threshold = 5
dtm_5<-dfm_trim(dtm_reduced, min_docfreq = 5)
df<-convert(dtm_5, to = "data.frame")
df<-subset(df,select=-c(doc_id))
df$num_words <- rowSums(df)
df_indicator<-df
df_indicator[,-which(names(df_indicator) %in% c('num_words'))]<-
  ifelse(df_indicator[,-which(names(df_indicator) %in% c('num_words'))]>=1,1,0)
df<-cbind(df,label=data$Review)
smp_siz <- floor(0.5*nrow(df))
index <- sample(seq_len(nrow(df)),size = smp_siz)
df_train_count <- df[index, ]
df_test_count <- df[-index, ]
df_indicator<-cbind(df_indicator,label=data$Review)
df_train_indicator<-df_indicator[index,]
df_test_indicator<-df_indicator[-index,]
colnames(df_train_count) <- paste(colnames(df_train_count), "_c", sep = "")
colnames(df_test_count) <- paste(colnames(df_test_count), "_c", sep = "")
colnames(df_train_indicator) <- paste(colnames(df_train_indicator), "_c",sep="")
colnames(df_test_indicator) <- paste(colnames(df_test_indicator), "_c", sep="")
rf_count <- randomForest(df_train_count$label_c~., data=df_train_count)
rf_indicator<-randomForest(df_train_indicator$label_c~.,data=df_train_indicator)
count_pred<- predict(rf_count, df_test_count)
indicator_pred<- predict(rf_indicator, df_test_indicator)
pred1<-round(count_pred)
pred1<-as.factor(pred1)
result1<-as.factor(df_test_count$label_c)
count_cm1<-confusionMatrix(pred1,result1)
print(paste0("Random forest - Count N-grams - Threshold = 5 - Accuracy = ",
             count_cm1$overall[1]))
pred2<-round(indicator_pred)
pred2<-as.factor(pred2)
result2<-as.factor(df_test_indicator$label_c)
indicator_cm1<-confusionMatrix(pred2,result2)
print(paste0("Random forest - Indicator N-grams - Threshold = 5 - Accuracy = ",indicator_cm1$overall[1]))
```

*Comments:*
From the above accuracies we can observe that for both the N-gram count and 
N-gram indicators models, their accuracies only vary by 1% with indicator 
performing better than count. This could be attributed to the fact that, since 
each unigram occurs with a low frequency, the model is unable to distinguish 
between common and rare words (due to threshold = 5), for N-gram count. For 
N-gram indicator, it doesn't have to worry about this as it only consists of 1's
and 0's. Hence, the indicator N-gram model performs very slightly better than 
the count N-gram model.


#b)
```{r}
#Threshold = 3
dtm_3<-dfm_trim(dtm_reduced, min_docfreq = 3)
df<-convert(dtm_3, to = "data.frame")
df<-subset(df,select=-c(doc_id))
df$num_words <- rowSums(df)
df_indicator<-df
df_indicator[,-which(names(df_indicator) %in% c('num_words'))]<-
  ifelse(df_indicator[,-which(names(df_indicator) %in% c('num_words'))]>=1,1,0)
df<-cbind(df,label=data$Review)
smp_siz <- floor(0.5*nrow(df))
index <- sample(seq_len(nrow(df)),size = smp_siz)
df_train_count <- df[index, ]
df_test_count <- df[-index, ]
df_indicator<-cbind(df_indicator,label=data$Review)
df_train_indicator<-df_indicator[index,]
df_test_indicator<-df_indicator[-index,]
colnames(df_train_count)<-paste(colnames(df_train_count), "_c", sep = "")
colnames(df_test_count)<-paste(colnames(df_test_count), "_c", sep = "")
colnames(df_train_indicator)<-paste(colnames(df_train_indicator), "_c",sep="")
colnames(df_test_indicator)<-paste(colnames(df_test_indicator),"_c",sep="")
rf_count <- randomForest(df_train_count$label_c~., data=df_train_count)
rf_indicator<-randomForest(df_train_indicator$label_c~.,data=df_train_indicator)
count_pred<- predict(rf_count, df_test_count)
indicator_pred<- predict(rf_indicator, df_test_indicator)
pred3<-round(count_pred)
pred3<-as.factor(pred3)
result3<-as.factor(df_test_count$label_c)
count_cm2<-confusionMatrix(pred3,result3)
print(paste0("Random forest - Count N-grams - Threshold = 3 - Accuracy = ",
             count_cm2$overall[1]))
pred4<-round(indicator_pred)
pred4<-as.factor(pred4)
result4<-as.factor(df_test_indicator$label_c)
indicator_cm2<-confusionMatrix(pred4,result4)
print(paste0("Random forest - Indicator N-grams - Threshold = 3 - Accuracy = ",indicator_cm2$overall[1]))
```

```{r}
#Threshold = 7
dtm_7<-dfm_trim(dtm_reduced, min_docfreq = 7)
df<-convert(dtm_7, to = "data.frame")
df<-subset(df,select=-c(doc_id))
df$num_words <- rowSums(df)
df_indicator<-df
df_indicator[,-which(names(df_indicator) %in% c('num_words'))]<-
  ifelse(df_indicator[,-which(names(df_indicator) %in% c('num_words'))]>=1,1,0)
df<-cbind(df,label=data$Review)
smp_siz <- floor(0.5*nrow(df))
index <- sample(seq_len(nrow(df)),size = smp_siz)
df_train_count <- df[index, ]
df_test_count <- df[-index, ]
df_indicator<-cbind(df_indicator,label=data$Review)
df_train_indicator<-df_indicator[index,]
df_test_indicator<-df_indicator[-index,]
colnames(df_train_count)<-paste(colnames(df_train_count), "_c", sep = "")
colnames(df_test_count)<-paste(colnames(df_test_count), "_c", sep = "")
colnames(df_train_indicator)<-paste(colnames(df_train_indicator), "_c",sep="")
colnames(df_test_indicator)<-paste(colnames(df_test_indicator),"_c",sep="")
rf_count <- randomForest(df_train_count$label_c~., data=df_train_count)
rf_indicator<-randomForest(df_train_indicator$label_c~.,data=df_train_indicator)
count_pred<- predict(rf_count, df_test_count)
indicator_pred<- predict(rf_indicator, df_test_indicator)
pred5<-round(count_pred)
pred5<-as.factor(pred5)
result5<-as.factor(df_test_count$label_c)
count_cm3<-confusionMatrix(pred5,result5)
print(paste0("Random forest - Count N-grams - Threshold = 7 - Accuracy = ",
             count_cm3$overall[1]))
pred6<-round(indicator_pred)
pred6<-as.factor(pred6)
result6<-as.factor(df_test_indicator$label_c)
indicator_cm3<-confusionMatrix(pred6,result6)
print(paste0("Random forest - Indicator N-grams - Threshold = 7 - Accuracy = ",
             indicator_cm3$overall[1]))
```

```{r}
#Threshold = 9
dtm_9<-dfm_trim(dtm_reduced, min_docfreq = 9)
df<-convert(dtm_9, to = "data.frame")
df<-subset(df,select=-c(doc_id))
df$num_words <- rowSums(df)
df_indicator<-df
df_indicator[,-which(names(df_indicator) %in% c('num_words'))]<-
  ifelse(df_indicator[,-which(names(df_indicator) %in% c('num_words'))]>=1,1,0)
df<-cbind(df,label=data$Review)
smp_siz <- floor(0.5*nrow(df))
index <- sample(seq_len(nrow(df)),size = smp_siz)
df_train_count <- df[index, ]
df_test_count <- df[-index, ]
df_indicator<-cbind(df_indicator,label=data$Review)
df_train_indicator<-df_indicator[index,]
df_test_indicator<-df_indicator[-index,]
colnames(df_train_count)<-paste(colnames(df_train_count), "_c", sep = "")
colnames(df_test_count)<-paste(colnames(df_test_count), "_c", sep = "")
colnames(df_train_indicator)<-paste(colnames(df_train_indicator), "_c",sep="")
colnames(df_test_indicator)<-paste(colnames(df_test_indicator),"_c",sep="")
rf_count <- randomForest(df_train_count$label_c~., data=df_train_count)
rf_indicator<-randomForest(df_train_indicator$label_c~.,data=df_train_indicator)
count_pred<- predict(rf_count, df_test_count)
indicator_pred<- predict(rf_indicator, df_test_indicator)
pred7<-round(count_pred)
pred7<-as.factor(pred7)
result7<-as.factor(df_test_count$label_c)
count_cm4<-confusionMatrix(pred7,result7)
print(paste0("Random forest - Count N-grams - Threshold = 9 - Accuracy = ",
             count_cm4$overall[1]))
pred8<-round(indicator_pred)
pred8<-as.factor(pred8)
result8<-as.factor(df_test_indicator$label_c)
indicator_cm4<-confusionMatrix(pred8,result8)
print(paste0("Random forest - Indicator N-grams - Threshold = 9 - Accuracy = ",indicator_cm4$overall[1]))
```

```{r}
#install.packages("knitr")
library(knitr)
Table1<-data.frame(Threshold=c(3,5,7,9),
                   Accuracy_count=c(count_cm2$overall[1],count_cm1$overall[1],
                                    count_cm3$overall[1],count_cm4$overall[1]),
                   Accuracy_indicator=c(indicator_cm2$overall[1],
                                        indicator_cm1$overall[1],
                                        indicator_cm3$overall[1],
                                        indicator_cm4$overall[1])
)
kable(Table1)

```

*Comments:*
In our question above, we can observe that as we increase the threshold, the 
accuracy for both N-gram count and N-gram indicator model reduces. This could
be due to the fact that - Using a minimum threshold is effective in reducing the
number of variables.N-gram model that occur only a few times across all texts 
are probably not helpful for prediction and can be discarded. However, setting 
the threshold too high may discard informative variables. We notice that when 
threshold = 3, we have the highest accuracy for both count and indicator model,
and as we increase the threshold to 9, accuracy drops. But, in all cases we can 
observe that accuracy of N-gram indicator model has a slightly better accuracy 
than the that of N-gram count.


#c)
```{r}
#With stem + without stop words
print(paste0("Count N-grams - With stemming, without stop words - Accuracy= ",
             count_cm1$overall[1]))
print(paste0("Indicator N-grams - With stemming, without stop words - Accuracy=",
             indicator_cm1$overall[1]))
```


```{r}
#With stem + with stop words
toks <- tokens(x=text,what="word",remove_punct=TRUE,remove_symbols=TRUE,
               remove_numbers=TRUE,remove_url=TRUE)
toks<-tokens_tolower(toks)
toks<-tokens_wordstem(toks)
#sw <- stopwords("english")
#toks<-tokens_remove(toks, sw)
dtm <- dfm(toks,tolower=TRUE,stem=TRUE)
sparsity(dtm)
summary(colSums(dtm))
tf_total <- colSums(dtm)
to_keep <- which(tf_total > median(tf_total))
dtm_reduced <- dfm_keep(dtm, pattern = names(to_keep), valuetype = "fixed", 
                        verbose = TRUE)
dfreq <- apply(dtm_reduced, 2, function(x) sum(x>0))
summary(dfreq)

#Threshold = 5
dtm_5<-dfm_trim(dtm_reduced, min_docfreq = 5)
df<-convert(dtm_5, to = "data.frame")
df<-subset(df,select=-c(doc_id))
df$num_words <- rowSums(df)
df_indicator<-df
df_indicator[,-which(names(df_indicator) %in% c('num_words'))]<-
  ifelse(df_indicator[,-which(names(df_indicator) %in% c('num_words'))]>=1,1,0)
df<-cbind(df,label=data$Review)
smp_siz <- floor(0.5*nrow(df))
index <- sample(seq_len(nrow(df)),size = smp_siz)
df_train_count <- df[index, ]
df_test_count <- df[-index, ]
df_indicator<-cbind(df_indicator,label=data$Review)
df_train_indicator<-df_indicator[index,]
df_test_indicator<-df_indicator[-index,]
colnames(df_train_count) <- paste(colnames(df_train_count), "_c", sep = "")
colnames(df_test_count) <- paste(colnames(df_test_count), "_c", sep = "")
colnames(df_train_indicator) <- paste(colnames(df_train_indicator), "_c",sep="")
colnames(df_test_indicator) <- paste(colnames(df_test_indicator), "_c", sep="")
rf_count <- randomForest(df_train_count$label_c~., data=df_train_count)
rf_indicator<-randomForest(df_train_indicator$label_c~.,data=df_train_indicator)
count_pred<- predict(rf_count, df_test_count)
indicator_pred<- predict(rf_indicator, df_test_indicator)
pred9<-round(count_pred)
pred9<-as.factor(pred9)
result9<-as.factor(df_test_count$label_c)
count_cm5<-confusionMatrix(pred9,result9)
print(paste0("Count N-grams - With stemming, with stop words - Accuracy= ",
             count_cm5$overall[1]))
pred10<-round(indicator_pred)
pred10<-as.factor(pred10)
result10<-as.factor(df_test_indicator$label_c)
indicator_cm5<-confusionMatrix(pred10,result10)
print(paste0("Count N-grams - With stemming, with stop words - Accuracy= ", 
             indicator_cm5$overall[1]))
```

```{r}
#Without stem + without stop words
toks <- tokens(x=text,what="word",remove_punct=TRUE,remove_symbols=TRUE,
               remove_numbers=TRUE,remove_url=TRUE)
toks<-tokens_tolower(toks)
#toks<-tokens_wordstem(toks)
sw <- stopwords("english")
toks<-tokens_remove(toks, sw)
dtm <- dfm(toks,tolower=TRUE,remove=sw)
sparsity(dtm)
summary(colSums(dtm))
tf_total <- colSums(dtm)
to_keep <- which(tf_total > median(tf_total))
dtm_reduced <- dfm_keep(dtm, pattern = names(to_keep), valuetype = "fixed", 
                        verbose = TRUE)
dfreq <- apply(dtm_reduced, 2, function(x) sum(x>0))
summary(dfreq)

#Threshold = 5
dtm_5<-dfm_trim(dtm_reduced, min_docfreq = 5)
df<-convert(dtm_5, to = "data.frame")
df<-subset(df,select=-c(doc_id))
df$num_words <- rowSums(df)
df_indicator<-df
df_indicator[,-which(names(df_indicator) %in% c('num_words'))]<-
  ifelse(df_indicator[,-which(names(df_indicator) %in% c('num_words'))]>=1,1,0)
df<-cbind(df,label=data$Review)
smp_siz <- floor(0.5*nrow(df))
index <- sample(seq_len(nrow(df)),size = smp_siz)
df_train_count <- df[index, ]
df_test_count <- df[-index, ]
df_indicator<-cbind(df_indicator,label=data$Review)
df_train_indicator<-df_indicator[index,]
df_test_indicator<-df_indicator[-index,]
colnames(df_train_count) <- paste(colnames(df_train_count), "_c", sep = "")
colnames(df_test_count) <- paste(colnames(df_test_count), "_c", sep = "")
colnames(df_train_indicator) <- paste(colnames(df_train_indicator), "_c",sep="")
colnames(df_test_indicator) <- paste(colnames(df_test_indicator), "_c", sep="")
rf_count <- randomForest(df_train_count$label_c~., data=df_train_count)
rf_indicator<-randomForest(df_train_indicator$label_c~.,data=df_train_indicator)
count_pred<- predict(rf_count, df_test_count)
indicator_pred<- predict(rf_indicator, df_test_indicator)
pred11<-round(count_pred)
pred11<-as.factor(pred11)
result11<-as.factor(df_test_count$label_c)
count_cm6<-confusionMatrix(pred11,result11)
print(paste0("Count N-grams - Without stemming,without stop words - Accuracy= ",
             count_cm6$overall[1]))
pred12<-round(indicator_pred)
pred12<-as.factor(pred12)
result12<-as.factor(df_test_indicator$label_c)
indicator_cm6<-confusionMatrix(pred12,result12)
print(paste0("Count N-grams - Without stemming,without stop words - Accuracy= ", 
             indicator_cm6$overall[1]))
```

```{r}
#Without stem + with stop words
toks <- tokens(x=text,what="word",remove_punct=TRUE,remove_symbols=TRUE,
               remove_numbers=TRUE,remove_url=TRUE)
toks<-tokens_tolower(toks)
#toks<-tokens_wordstem(toks)
#sw <- stopwords("english")
#toks<-tokens_remove(toks, sw)
dtm <- dfm(toks,tolower=TRUE)
sparsity(dtm)
summary(colSums(dtm))
tf_total <- colSums(dtm)
to_keep <- which(tf_total > median(tf_total))
dtm_reduced <- dfm_keep(dtm, pattern = names(to_keep), valuetype = "fixed", 
                        verbose = TRUE)
dfreq <- apply(dtm_reduced, 2, function(x) sum(x>0))
summary(dfreq)

#Threshold = 5
dtm_5<-dfm_trim(dtm_reduced, min_docfreq = 5)
df<-convert(dtm_5, to = "data.frame")
df<-subset(df,select=-c(doc_id))
df$num_words <- rowSums(df)
df_indicator<-df
df_indicator[,-which(names(df_indicator) %in% c('num_words'))]<-
  ifelse(df_indicator[,-which(names(df_indicator) %in% c('num_words'))]>=1,1,0)
df<-cbind(df,label=data$Review)
smp_siz <- floor(0.5*nrow(df))
index <- sample(seq_len(nrow(df)),size = smp_siz)
df_train_count <- df[index, ]
df_test_count <- df[-index, ]
df_indicator<-cbind(df_indicator,label=data$Review)
df_train_indicator<-df_indicator[index,]
df_test_indicator<-df_indicator[-index,]
colnames(df_train_count) <- paste(colnames(df_train_count), "_c", sep = "")
colnames(df_test_count) <- paste(colnames(df_test_count), "_c", sep = "")
colnames(df_train_indicator) <- paste(colnames(df_train_indicator), "_c",sep="")
colnames(df_test_indicator) <- paste(colnames(df_test_indicator), "_c", sep="")
rf_count <- randomForest(df_train_count$label_c~., data=df_train_count)
rf_indicator<-randomForest(df_train_indicator$label_c~.,data=df_train_indicator)
count_pred<- predict(rf_count, df_test_count)
indicator_pred<- predict(rf_indicator, df_test_indicator)
pred13<-round(count_pred)
pred13<-as.factor(pred13)
result13<-as.factor(df_test_count$label_c)
count_cm7<-confusionMatrix(pred13,result13)
print(paste0("Count N-grams - Without stemming,with stop words - Accuracy= ",
             count_cm7$overall[1]))
pred14<-round(indicator_pred)
pred14<-as.factor(pred14)
result14<-as.factor(df_test_indicator$label_c)
indicator_cm7<-confusionMatrix(pred14,result14)
print(paste0("Count N-grams - Without stemming,with stop words - Accuracy= ", 
             indicator_cm7$overall[1]))
```

```{r}
Table2<-data.frame(Condition=c('With stem + without stop words',
                               'With stem + with stop words',
                               'Without stem + without stop words',
                               'Without stem + with stop words'),
                   Accuracy_count=c(count_cm1$overall[1],count_cm5$overall[1],
                                    count_cm6$overall[1],count_cm7$overall[1]),
                   Accuracy_indicator=c(indicator_cm1$overall[1],
                                        indicator_cm5$overall[1],
                                        indicator_cm6$overall[1],
                                        indicator_cm7$overall[1])
)
kable(Table2)
```
*Comments:*
From the above table we can observe that stemming is very useful and helps
increase the accuracy. With stemming (with/without stop words) models - have the 
best accuracies. Whereas, we can observe that if we include stop words, the
number of variables created increase.
With stemming and with stop words - 733 features were kept and
without stemming and with stop words - 789 features were kept.
But without stop words, 
With stemming without stop words - 643 features were kept and
without stemming without stop words - 683 features were kept.
So we can see that withour stopwords we have lesser features created it, and 
therefore keeping stop words increases the number of features and reduces the
accuracy. Whereas, with stemming we observe higher accuracies. The highest
accuracy is observed with stemming and without stop words.


```{r}
library(glmnet)
text<-data$Text
toks <- tokens(x=text,what="word",remove_punct=TRUE,remove_symbols=TRUE,
               remove_numbers=TRUE,remove_url=TRUE)
toks<-tokens_tolower(toks)
toks<-tokens_wordstem(toks)
sw <- stopwords("english")
toks<-tokens_remove(toks, sw)
dtm <- dfm(toks,tolower=TRUE,stem=TRUE,remove=sw)
sparsity(dtm)
summary(colSums(dtm))
tf_total <- colSums(dtm)
to_keep <- which(tf_total > median(tf_total))
dtm_reduced <- dfm_keep(dtm, pattern = names(to_keep), valuetype = "fixed", 
                        verbose = TRUE)
dfreq <- apply(dtm_reduced, 2, function(x) sum(x>0))
summary(dfreq)

#Threshold = 5
dtm_5_lr<-dfm_trim(dtm_reduced, min_docfreq = 5)
df<-convert(dtm_5_lr, to = "data.frame")
df<-subset(df,select=-c(doc_id))
df$num_words <- rowSums(df)
df_indicator<-df
df_indicator[,-which(names(df_indicator) %in% c('num_words'))]<-
  ifelse(df_indicator[,-which(names(df_indicator) %in% c('num_words'))]>=1,1,0)
df<-cbind(df,label=data$Review)
smp_siz <- floor(0.5*nrow(df))
index <- sample(seq_len(nrow(df)),size = smp_siz)
df_train_count <- df[index, ]
df_test_count <- df[-index, ]
df_indicator<-cbind(df_indicator,label=data$Review)
df_train_indicator<-df_indicator[index,]
df_test_indicator<-df_indicator[-index,]

lr_count<-glm(df_train_count$label~.,data=df_train_count,family="binomial")
lr_indicator<-glm(df_train_indicator$label~.,data=df_train_indicator,
                  family="binomial")

count_pred1<- predict(lr_count, df_test_count)
count_pred1 <- ifelse(count_pred1 >= 0.5, 1, 0)
indicator_pred1<- predict(lr_indicator, df_test_indicator)
indicator_pred1<- ifelse(indicator_pred1 <= 0.5, 1, 0)

pred15<-round(count_pred1)
pred15<-as.factor(pred15)
result15<-as.factor(df_test_count$label)
count_cm8<-confusionMatrix(pred15,result15)
print(paste0("Logistic regression - Count N-grams - Threshold = 5 - Accuracy= ",
             count_cm8$overall[1]))
pred16<-round(indicator_pred1)
pred16<-as.factor(pred16)
result16<-as.factor(df_test_indicator$label)
indicator_cm8<-confusionMatrix(pred16,result16)
print(paste0("Logistic regression - Indicator N-grams - Threshold = 5 - Accuracy = ",indicator_cm8$overall[1]))
```

```{r}
Table3<-data.frame(LR_accuracy_count=c(count_cm8$overall[1]),
                   LR_accuracy_indicator=c(indicator_cm8$overall[1])
)
kable(Table3)
```
*Comments:*
From this above model using logistic regression we can observe that out accuracy
is much worse compared to the random forest models. This can be explained using
overfitting, logistic regression models tend to overfit and hence reduce the 
accuracy of the model. This could also be due to the collinearity between the
variables. 
